var documenterSearchIndex = {"docs":
[{"location":"static_neural_network_parameters/#Static-Neural-Network-Parameters","page":"Static Neural Network Parameters","title":"Static Neural Network Parameters","text":"","category":"section"},{"location":"static_neural_network_parameters/","page":"Static Neural Network Parameters","title":"Static Neural Network Parameters","text":"We can also allocate neural network parameters using StaticArrays. Therefore we simply need to set the keyword static to true in the NeuralNetwork constructor. ","category":"page"},{"location":"static_neural_network_parameters/","page":"Static Neural Network Parameters","title":"Static Neural Network Parameters","text":"warning: Warning\nStatic neural network parameters are only supported for dense CPU arrays. AbstractNeuralNetworks defines a type CPUStatic, but does not have equivalent GPU objects.","category":"page"},{"location":"static_neural_network_parameters/","page":"Static Neural Network Parameters","title":"Static Neural Network Parameters","text":"using AbstractNeuralNetworks\nimport Random\nRandom.seed!(123)\n\nbackend = AbstractNeuralNetworks.CPUStatic()\ninput_dim = 2\nn_hidden_layers = 100\nc = Chain(Dense(input_dim, 10, tanh), Tuple(Dense(10, 10, tanh) for _ in 1:n_hidden_layers)..., Dense(10, 1, tanh))\nnn = NeuralNetwork(c, backend)\ntypeof(params(nn).L1.W)","category":"page"},{"location":"static_neural_network_parameters/","page":"Static Neural Network Parameters","title":"Static Neural Network Parameters","text":"We can compare different evaluation times:","category":"page"},{"location":"static_neural_network_parameters/","page":"Static Neural Network Parameters","title":"Static Neural Network Parameters","text":"nn_cpu = changebackend(CPU(), nn)\nsecond_dim = 200\nx = rand(input_dim, second_dim)\nnn(x); # hide\n@time nn(x);\nnothing # hide","category":"page"},{"location":"static_neural_network_parameters/","page":"Static Neural Network Parameters","title":"Static Neural Network Parameters","text":"nn_cpu(x); # hide\n@time nn_cpu(x);\nnothing # hide","category":"page"},{"location":"static_neural_network_parameters/","page":"Static Neural Network Parameters","title":"Static Neural Network Parameters","text":"If we also make the input static, we get:","category":"page"},{"location":"static_neural_network_parameters/","page":"Static Neural Network Parameters","title":"Static Neural Network Parameters","text":"using StaticArrays\nx = @SMatrix rand(input_dim, second_dim)\nnn(x);\n@time nn(x);\nnothing # hide","category":"page"},{"location":"static_neural_network_parameters/","page":"Static Neural Network Parameters","title":"Static Neural Network Parameters","text":"nn_cpu(x); # hide\n@time nn_cpu(x);\nnothing # hide","category":"page"},{"location":"bibliography/#References","page":"References","title":"References","text":"","category":"section"},{"location":"bibliography/","page":"References","title":"References","text":"X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In: Proceedings of the thirteenth international conference on artificial intelligence and statistics (JMLR Workshop and Conference Proceedings, 2010); pp. 249–256.\n\n\n\n","category":"page"},{"location":"#AbstractNeuralNetworks","page":"Home","title":"AbstractNeuralNetworks","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for AbstractNeuralNetworks.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#AbstractNeuralNetworks.NeuralNetworkBackend","page":"Home","title":"AbstractNeuralNetworks.NeuralNetworkBackend","text":"NeuralNetworkBackend\n\nThe backend that specifies where and how neural network parameters are allocated.\n\nIt largely inherits properties from KernelAbstractions.Backend, but also adds CPUStatic which is defined in AbstractNeuralNetworks.\n\n\n\n\n\n","category":"type"},{"location":"#AbstractNeuralNetworks.AbstractCell","page":"Home","title":"AbstractNeuralNetworks.AbstractCell","text":"AbstractCell\n\nAn AbstractCell is a map from mathbbR^MmathbbR^N rightarrow mathbbR^OmathbbR^P.\n\nConcrete cell types should implement the following functions:\n\ninitialparameters(backend::NeuralNetworkBackend, ::Type{T}, cell::AbstractCell; init::Initializer = default_initializer(), rng::AbstractRNG = Random.default_rng())\nupdate!(::AbstractLayer, θ::NamedTuple, dθ::NamedTuple, η::AbstractFloat)\n\nand the functors\n\ncell(x, st, ps)\ncell(z, y, x, st, ps)\n\n\n\n\n\n","category":"type"},{"location":"#AbstractNeuralNetworks.AbstractExplicitCell","page":"Home","title":"AbstractNeuralNetworks.AbstractExplicitCell","text":"AbstractExplicitCell\n\nAbstract supertype for explicit cells. This type exists mainly for compatibility with Lux.\n\n\n\n\n\n","category":"type"},{"location":"#AbstractNeuralNetworks.AbstractExplicitLayer","page":"Home","title":"AbstractNeuralNetworks.AbstractExplicitLayer","text":"AbstractExplicitLayer\n\nAbstract supertype for explicit layers. This type exists mainly for compatibility with Lux.\n\n\n\n\n\n","category":"type"},{"location":"#AbstractNeuralNetworks.AbstractLayer","page":"Home","title":"AbstractNeuralNetworks.AbstractLayer","text":"AbstractLayer\n\nAn AbstractLayer is a map from mathbbR^M rightarrow mathbbR^N.\n\nConcrete layer types should implement the following functions:\n\ninitialparameters(backend::NeuralNetworkBackend, ::Type{T}, layer::AbstractLayer; init::Initializer = default_initializer(), rng::AbstractRNG = Random.default_rng())\nupdate!(::AbstractLayer, θ::NamedTuple, dθ::NamedTuple, η::AbstractFloat)\n\nand the functors\n\nlayer(x, ps)\nlayer(y, x, ps)\n\n\n\n\n\n","category":"type"},{"location":"#AbstractNeuralNetworks.AbstractPullback","page":"Home","title":"AbstractNeuralNetworks.AbstractPullback","text":"AbstractPullback{NNLT<:NetworkLoss}\n\nAbstractPullback is an abstract type that encompasses all ways of performing differentiation (especially computing the gradient with respect to neural network parameters) in GeometricMachineLearning.\n\nIf a user wants to implement a custom Pullback the following two functions have to be extended:\n\n(_pullback::AbstractPullback)(ps, model, input_nt_output_nt::Tuple{<:QPTOAT, <:QPTOAT})\n(_pullback::AbstractPullback)(ps, model, input_nt::QPT)\n\nbased on the loss::NetworkLoss that's stored in _pullback. The output of _pullback needs to be a Tuple that contains:\n\nthe loss evaluated at ps and input_nt (or input_nt_output_nt),\nthe gradient of loss with respect to ps that call be called with e.g.:\n\n_pullback(ps, model, input_nt)[2](1) # returns the gradient wrt to `ps`\n\nldots we use this convention as it is analogous to how Zygote builds pullbacks.\n\nAn example is GeometricMachineLearning.ZygotePullback.\n\n\n\n\n\n","category":"type"},{"location":"#AbstractNeuralNetworks.Architecture","page":"Home","title":"AbstractNeuralNetworks.Architecture","text":"Architecture\n\n\n\n\n\n","category":"type"},{"location":"#AbstractNeuralNetworks.CPUStatic","page":"Home","title":"AbstractNeuralNetworks.CPUStatic","text":"CPUStatic\n\nAn additional backend that specifies allocation of static arrays.\n\nImplementation\n\nThis is not a subtype of KernelAbstractions.Backend as it is associated with StaticArrays.MArray and such subtyping would therefore constitute type piracy.\n\n\n\n\n\n","category":"type"},{"location":"#AbstractNeuralNetworks.Chain","page":"Home","title":"AbstractNeuralNetworks.Chain","text":"Chain\n\nA chain is a sequence of layers.\n\nA Chain can be initialized by passing an arbitrary number of layers\n\nChain(layers...)\n\nor a neural network architecture together with a backend and a parameter type:\n\nChain(::Architecture, ::NeuralNetworkBackend, ::Type; kwargs...)\nChain(::Architecture, ::Type; kwargs...)\n\nIf the backend is omitted, the default backend CPU() is chosen. The keyword arguments will be passed to the initialparameters method of each layer.\n\n\n\n\n\n","category":"type"},{"location":"#AbstractNeuralNetworks.FeedForwardLoss","page":"Home","title":"AbstractNeuralNetworks.FeedForwardLoss","text":"FeedForwardLoss()\n\nMake an instance of a loss for feedforward neural networks.\n\nThis should be used together with a neural network of type GeometricMachineLearning.NeuralNetworkIntegrator.\n\nExample\n\nFeedForwardLoss applies a neural network to an input and compares it to the output via an L_2 norm:\n\nusing AbstractNeuralNetworks\nusing LinearAlgebra: norm\nimport Random\nRandom.seed!(123)\n\nconst d = 2\narch = Chain(Dense(d, d), Dense(d, d))\nnn = NeuralNetwork(arch)\n\ninput_vec =  [1., 2.]\noutput_vec = [3., 4.]\nloss = FeedForwardLoss()\n\nloss(nn, input_vec, output_vec) ≈ norm(output_vec - nn(input_vec)) / norm(output_vec)\n\n# output\n\ntrue\n\nSo FeedForwardLoss simply does:\n\n    mathttloss(mathcalNN mathttinput mathttoutput) =  mathcalNN(mathttinput) - mathttoutput    mathttoutput\n\nwhere cdot is the L_2 norm. \n\nParameters\n\nThis loss does not have any parameters.\n\n\n\n\n\n","category":"type"},{"location":"#AbstractNeuralNetworks.GlorotUniform","page":"Home","title":"AbstractNeuralNetworks.GlorotUniform","text":"GlorotUniform <: Initializer\n\nGlorot uniform was introduced by [1].\n\n\n\n\n\n","category":"type"},{"location":"#AbstractNeuralNetworks.Initializer","page":"Home","title":"AbstractNeuralNetworks.Initializer","text":"Initializer\n\nDetermines how neural network weights are initialized.\n\n\n\n\n\n","category":"type"},{"location":"#AbstractNeuralNetworks.Model","page":"Home","title":"AbstractNeuralNetworks.Model","text":"A supertype for Chain, AbstractCell etc.\n\n\n\n\n\n","category":"type"},{"location":"#AbstractNeuralNetworks.NetworkLoss","page":"Home","title":"AbstractNeuralNetworks.NetworkLoss","text":"NetworkLoss\n\nAn abstract type for all the neural network losses.  If you want to implement CustomLoss <: NetworkLoss you need to define a functor:\n\n(loss::CustomLoss)(model, ps, input, output)\n\nwhere model is an instance of an AbstractExplicitLayer or a Chain and ps the parameters.\n\nSee FeedForwardLoss, GeometricMachineLearning.TransformerLoss, GeometricMachineLearning.AutoEncoderLoss and GeometricMachineLearning.ReducedLoss for examples.\n\n\n\n\n\n","category":"type"},{"location":"#AbstractNeuralNetworks.NeuralNetwork","page":"Home","title":"AbstractNeuralNetworks.NeuralNetwork","text":"NeuralNetwork <: AbstractNeuralNetwork\n\nNeuralnetwork stores the Architecture, Model, neural network paramters and backend of the system.\n\nImplementation\n\nSee NeuralNetworkBackend for the backend.\n\n\n\n\n\n","category":"type"},{"location":"#AbstractNeuralNetworks.NeuralNetworkParameters","page":"Home","title":"AbstractNeuralNetworks.NeuralNetworkParameters","text":"NeuralNetworkParameters\n\nThis struct stores the parameters of a neural network. In essence, it is just a wrapper around a NamedTuple of NamedTuple that provides some context, e.g., for storing parameters to file.\n\n\n\n\n\n","category":"type"},{"location":"#AbstractNeuralNetworks.OneInitializer","page":"Home","title":"AbstractNeuralNetworks.OneInitializer","text":"OneInitializer <: Initializer\n\n\n\n\n\n","category":"type"},{"location":"#AbstractNeuralNetworks.ZeroInitializer","page":"Home","title":"AbstractNeuralNetworks.ZeroInitializer","text":"ZeroInitializer <: Initializer\n\n\n\n\n\n","category":"type"},{"location":"#AbstractNeuralNetworks.apply!-Tuple{AbstractArray, AbstractArray, AbstractNeuralNetworks.AbstractLayer, Any, Any}","page":"Home","title":"AbstractNeuralNetworks.apply!","text":"apply!(y, cell::AbstractCell, x, ps)\n\nSimply calls cell(y, x, ps)\n\n\n\n\n\n","category":"method"},{"location":"#AbstractNeuralNetworks.apply!-Tuple{AbstractArray, AbstractNeuralNetworks.AbstractLayer, Any, Any}","page":"Home","title":"AbstractNeuralNetworks.apply!","text":"apply!(y, layer::AbstractLayer, x, ps)\n\nSimply calls layer(y, x, ps)\n\n\n\n\n\n","category":"method"},{"location":"#AbstractNeuralNetworks.apply-Tuple{AbstractNeuralNetworks.AbstractCell, Any, Any, Any}","page":"Home","title":"AbstractNeuralNetworks.apply","text":"apply(cayer::AbstractCell, x, ps)\n\nSimply calls cell(x, st, ps)\n\n\n\n\n\n","category":"method"},{"location":"#AbstractNeuralNetworks.apply-Tuple{AbstractNeuralNetworks.AbstractLayer, Any, Any}","page":"Home","title":"AbstractNeuralNetworks.apply","text":"apply(layer::AbstractLayer, x, ps)\n\nSimply calls layer(x, ps)\n\n\n\n\n\n","category":"method"},{"location":"#AbstractNeuralNetworks.changebackend-Tuple{NeuralNetworkBackend, NeuralNetwork}","page":"Home","title":"AbstractNeuralNetworks.changebackend","text":"changebackend(backend, nn)\n\nExtended help\n\nThe function changebackend is defined for NeuralNetworkParameters, NeuralNetwork, AbstractArrays and NamedTuples. This function is also exported.\n\n\n\n\n\n","category":"method"},{"location":"#AbstractNeuralNetworks.initialparameters","page":"Home","title":"AbstractNeuralNetworks.initialparameters","text":"initialparameters\n\nReturns the initial parameters of a model, i.e., a layer or chain.\n\ninitialparameters(backend::NeuralNetworkBackend, ::Type{T}, model::Model; init::Initializer = default_initializer(), rng::AbstractRNG = Random.default_rng())\ninitialparameters(::Type{T}, model::Model; init::Initializer = default_initializer(), rng::AbstractRNG = Random.default_rng())\n\nThe init! function must have the following signature:\n\ninit!(rng::AbstractRNG, x::AbstractArray)\n\nThe default_initializer() returns randn!.\n\n\n\n\n\n","category":"function"},{"location":"#AbstractNeuralNetworks.networkbackend-Tuple{AbstractArray}","page":"Home","title":"AbstractNeuralNetworks.networkbackend","text":"networkbackend(arr)\n\nReturns the NeuralNetworkBackend of arr.\n\n\n\n\n\n","category":"method"}]
}
